{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Crawling\n",
    "\n",
    "Get posts, post meta and comments from facebook fan pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_file = './src/chorme_driver/chromedriver' # point to the binary file where you store the chrome driver\n",
    "email = 'your_facebook_account' # your facebook account\n",
    "password = 'your_facebook_password'# your facebook password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fanpage name we want to search\n",
    "fanpage_name_list = ['ETtoday','YahooTWNews','appledaily.tw','news.ebc','setnews','tvbsfb','tvbsfb']\n",
    "# keywords for the posts we're search within the fanpage\n",
    "keyword_list = ['三倍券','振興券','消費券']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about xpath\n",
    "WebElement foo = driver.findElement(By.xpath(\"//*[@title='\" + title + \"']\"));\n",
    "WebElement bar = foo.findElement(By.xpath(\".//img\"));\n",
    "The leading dot in the second xpath makes all the difference, as it will search only for child elements of foo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login driver: go to facebook login page and type in account and password\n",
    "class loginDriver:\n",
    "    def __init__(self, driver_path):\n",
    "        options = Options()\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        self.driver = webdriver.Chrome(driver_path, options=options)\n",
    "    def login(self, email, password):\n",
    "        self.driver.get(\"https://www.facebook.com/\")\n",
    "        time.sleep(5)\n",
    "        email_elem = self.driver.find_element_by_id(\"email\")\n",
    "        password_elem = self.driver.find_element_by_id(\"pass\")\n",
    "        email_elem.send_keys(email)\n",
    "        password_elem.send_keys(password)\n",
    "        password_elem.submit()\n",
    "        time.sleep(5)\n",
    "        return self.driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbClawer, it could \n",
    "# (1) go the the given fan page, use the searching feature on facebook to find posts according to keyword \n",
    "# (2) get the post contents,meta data, reactions, comments  \n",
    "# yea I know the code is a mess. \n",
    "# While facebook may change layout structure from time to time, it's understandable the code won't last long\n",
    "class fbClawer:\n",
    "    def __init__(self, driver):\n",
    "        self.driver = driver\n",
    "    \n",
    "    def elem_click(self, elem):\n",
    "        webdriver.ActionChains(self.driver).move_to_element(elem).click(elem).perform()\n",
    "        time.sleep(np.random.rand()*2 + 2)\n",
    "    \n",
    "    def find_posts(self, fanpage_name, keyword, scroll_times):\n",
    "        url = f'https://www.facebook.com/{fanpage_name}/'\n",
    "        post_link_list = []\n",
    "        self.driver.get(url)\n",
    "        time.sleep(5)\n",
    "        search_button_element = self.driver.find_element_by_xpath('//div[@aria-label=\"Search\"]')\n",
    "        self.elem_click(search_button_element)\n",
    "\n",
    "        search_input_elem = self.driver.find_element_by_xpath('//input[@placeholder=\"Search this Page\"]')\n",
    "        search_input_elem.send_keys(keyword)\n",
    "        search_input_elem.send_keys(u'\\ue006') # press \"return\" key\n",
    "\n",
    "        for i in range(scroll_times):\n",
    "            self.driver.execute_script('window.scrollTo(10, document.body.scrollHeight);')\n",
    "            WebDriverWait(self.driver, 10)\n",
    "            time.sleep(np.random.randint(3)+2)\n",
    "\n",
    "        links_elem_list = self.driver.find_elements_by_xpath('//div[@role=\"feed\"]//a')\n",
    "\n",
    "        for link in links_elem_list:\n",
    "            href = link.get_attribute(\"href\") # innerHTML, outerHTML could get raw html\n",
    "            if href and 'posts' in href and url in href:\n",
    "                href = href.split(\"?\")[0]\n",
    "                post_link_list.append(href)\n",
    "        return list(set(post_link_list))\n",
    "    \n",
    "    def get_post_content(self, url):\n",
    "        print(url)\n",
    "        self.driver.get(url)\n",
    "        time.sleep(5)\n",
    "        re_dict = {'post_url':url}\n",
    "        \n",
    "        # the post is located under\n",
    "        post_elem_xpath = '//div[@aria-labelledby and @role=\"article\"]'\n",
    "        post_elem = self.driver.find_elements_by_xpath(post_elem_xpath)[0]\n",
    "\n",
    "        # post time\n",
    "        posttime_elem_list = self.driver.find_elements_by_xpath('//div[@hidden=\"true\"]//span')\n",
    "        posttime = [i.get_attribute(\"innerHTML\") for i in posttime_elem_list if not \"Sponsored\" in i.get_attribute(\"innerHTML\")][0]\n",
    "        re_dict['post_time'] = posttime\n",
    "\n",
    "        # post content\n",
    "        post_content_elem = post_elem.find_element_by_xpath('.//div[@data-ad-comet-preview=\"message\"]')\n",
    "        re_dict['post_content'] = post_content_elem.text\n",
    "\n",
    "        # post referlink text\n",
    "        post_referlink_elem_list = post_elem.find_elements_by_xpath('.//div[contains(@id,\"jsc\")]//a[@rel=\"nofollow noopener\" and @role=\"link\" and @href]//span[@dir=\"auto\"]')\n",
    "        if len(post_referlink_elem_list)>1:\n",
    "            referlink_text = post_referlink_elem_list[1].text.replace(\"\\u3000\",\"\")\n",
    "            re_dict['link_title'] = referlink_text\n",
    "        else:\n",
    "            re_dict['link_title'] = ''\n",
    "\n",
    "    #     reaction stats\n",
    "        total_reaction = post_elem.find_element_by_xpath('.//span[@aria-hidden=\"true\"]/span').text\n",
    "\n",
    "        reaction_stat_link_elem = post_elem.find_element_by_xpath('.//span[@aria-label=\"See who reacted to this\"]')\n",
    "        reaction_stat_elem_list = reaction_stat_link_elem.find_elements_by_xpath('.//div')\n",
    "        re_dict.update({'reaction_stat':\n",
    "                        {'total':total_reaction,\n",
    "                         'top_3_reaction':[i.get_attribute('aria-label') for i in reaction_stat_elem_list]\n",
    "                        }})\n",
    "\n",
    "        # comment\n",
    "        # 1 - change showing comment to all comment\n",
    "        try:\n",
    "            change_showing_comment_element = post_elem.find_element_by_xpath('.//div[@role=\"button\"]//*[contains(text(),\\'Most Relevant\\')]')\n",
    "        except:\n",
    "            self.dumpdata(url, re_dict)\n",
    "            return\n",
    "        self.elem_click(change_showing_comment_element)\n",
    "        allcomment_element = post_elem.find_element_by_xpath('//div[@tabindex=\"-1\"]//div[@data-pagelet=\"root\"]//div[@role=\"menu\"]//div[@role=\"menuitem\"]//span[@dir=\"auto\" and contains(text(),\"All Comments\")]')\n",
    "        self.elem_click(allcomment_element)\n",
    "\n",
    "        time.sleep(5)\n",
    "        # 2 - expanding all show more comments\n",
    "\n",
    "        comment_expand_times = 0\n",
    "        while True:\n",
    "            temp = post_elem.find_elements_by_xpath('.//div[@data-visualcompletion]')\n",
    "            # broute force to find main post section\n",
    "            for elem_ind in range(len(temp)):\n",
    "                if re.search(\"Write\\ a\\ comment\", temp[elem_ind].text):\n",
    "                    break\n",
    "            comment_sec_elem = temp[elem_ind]\n",
    "\n",
    "            has_match_morecomment = re.search(\"View\\ (\\d+\\ )?more\\ comments\", comment_sec_elem.text)\n",
    "            if has_match_morecomment:\n",
    "                print(f\"expanding comments {comment_expand_times}\")\n",
    "                comment_expand_times+=1\n",
    "                more_comment_button_elem = comment_sec_elem.find_element_by_xpath('.//*[contains(text(),\"'+has_match_morecomment.group(0)+'\")]')\n",
    "                self.elem_click(more_comment_button_elem)\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        comment_elem_list = comment_sec_elem.find_elements_by_xpath(\".//ul\")[1].find_elements_by_xpath(\".//li\")\n",
    "        print(f\"possilbe have {len(comment_elem_list)/5} comments\")\n",
    "\n",
    "        # 3 - expand all comments and reply, get the contents\n",
    "        comment_list = []\n",
    "        for elem in comment_elem_list:\n",
    "            comment_dict = {}\n",
    "            # sample comment \n",
    "            # 'Chun Steel\\n留個百年做紀念吧……\\n15\\nLike\\n · Reply · See Translation · 2h'\n",
    "            comment_elem = elem.find_elements_by_xpath('./div')\n",
    "\n",
    "            if(len(comment_elem)!=2):\n",
    "                continue\n",
    "                \n",
    "            # \"see more\" click out for main comment\n",
    "            comment_main_elem = comment_elem[0]\n",
    "            try:\n",
    "                see_more_button_elem = comment_main_elem.find_element_by_xpath('.//div[@role=\"button\" and contains(text(),\"See More\")]')\n",
    "                self.elem_click(see_more_button_elem)\n",
    "            except:\n",
    "                pass\n",
    "            comment_dict['main'] = comment_main_elem.text.split(\"\\nLike\")[0]\n",
    "\n",
    "            # replies\n",
    "            comment_reply_elem = comment_elem[1]\n",
    "            if len(comment_reply_elem.text)>0 and re.search(\"\\d+\\ Reply\", comment_reply_elem.text) or re.search(\"\\d+\\ Replies\", comment_reply_elem.text):\n",
    "                self.elem_click(comment_reply_elem)\n",
    "                flag = True\n",
    "                while flag:\n",
    "                    flag = False\n",
    "                    try:\n",
    "                        possible_more_reply_elem_list = comment_reply_elem.find_elements_by_xpath('.//span[contains(text(),\"View\")]')\n",
    "                        for i in range(len(possible_more_reply_elem_list)):\n",
    "                            if not \"Hide\" in possible_more_reply_elem_list[i].text:\n",
    "                                flag = True\n",
    "                                self.elem_click(possible_more_reply_elem_list[i])\n",
    "                    except Exception as e:\n",
    "                        flag = False\n",
    "\n",
    "                reply_elem_list = comment_reply_elem.find_elements_by_xpath('.//div[contains(@aria-label,\"Reply\")]')\n",
    "                for reply_elem in reply_elem_list:\n",
    "                    try:\n",
    "                        see_more_button_elem = reply_elem.find_element_by_xpath('.//div[@role=\"button\" and contains(text(),\"See More\")]')\n",
    "                        self.elem_click(see_more_button_elem)\n",
    "                    except:\n",
    "                        pass\n",
    "                comment_dict['replies'] = [reply.text.split(\"\\nLike\")[0] for reply in reply_elem_list[:-1]]\n",
    "\n",
    "            else:\n",
    "                comment_dict['replies'] = []\n",
    "            comment_list.append(comment_dict)\n",
    "        re_dict.update({'comments':comment_list})\n",
    "        \n",
    "        self.dumpdata(url, re_dict)\n",
    "        return\n",
    "        \n",
    "    def dumpdata(self, url, re_dict):\n",
    "        splitted_url = url.split(\"/posts/\")\n",
    "        with open(\"./data/\"+splitted_url[0].split(\"/\")[-1]+'-'+splitted_url[1], 'w') as outfile:\n",
    "            json.dump(re_dict, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydriver = loginDriver(driver_path=driver_file).login(email=email, password=password)\n",
    "crawler = fbClawer(mydriver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all the posts links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scroll_times = 20\n",
    "\n",
    "# postlinks = []\n",
    "# for fanpage_name in fanpage_name_list:\n",
    "#     for keyword in keyword_list:\n",
    "#         add_postlinks = crawler.find_posts(fanpage_name, keyword, scroll_times = scroll_times)\n",
    "#         print(f'{fanpage_name} {keyword} - {len(add_postlinks)}')\n",
    "#         postlinks+=add_postlinks\n",
    "# postlinks = set(postlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all post links\n",
    "postlink_ser = pd.Series(list(postlinks))\n",
    "postlink_ser.to_csv(\"./data/post_links.csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note**\n",
    "Total we would get 600+ posts related to the topic.   \n",
    "We still need to do some manuaul work here to check each post to exclude un-related posts. Save them into \"data/exclude_link.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each post, get the post content and comments and its reaction\n",
    "This may takes a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683\n"
     ]
    }
   ],
   "source": [
    "postlink_ser = pd.read_csv(\"./data/post_links.csv\")['0']\n",
    "skip_url_list = open(\"./data/exclude_link.txt\",'r').readlines()# links we don't want to crawl\n",
    "skip_url_list = [i.replace(\"\\n\",\"\") for i in skip_url_list]\n",
    "f.close()\n",
    "\n",
    "postlink_ser = postlink_ser[np.logical_not(postlink_ser.isin(skip_url_list))].reset_index(drop=True)\n",
    "print(len(postlink_ser))\n",
    "postlink_ser = postlink_ser.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.facebook.com/YahooTWNews/posts/3476696202379321\n",
      "expanding comments 0\n",
      "expanding comments 1\n",
      "possilbe have 83.0 comments\n",
      "https://www.facebook.com/YahooTWNews/posts/3464992330216375\n",
      "expanding comments 0\n",
      "expanding comments 1\n",
      "possilbe have 59.6 comments\n",
      "https://www.facebook.com/news.ebc/posts/4693202120714842\n",
      "expanding comments 0\n",
      "expanding comments 1\n",
      "expanding comments 2\n",
      "expanding comments 3\n",
      "expanding comments 4\n",
      "expanding comments 5\n",
      "possilbe have 298.0 comments\n",
      "https://www.facebook.com/YahooTWNews/posts/3607031006012506\n",
      "expanding comments 0\n",
      "expanding comments 1\n",
      "expanding comments 2\n",
      "expanding comments 3\n",
      "possilbe have 160.6 comments\n",
      "https://www.facebook.com/ETtoday/posts/3437448309624642\n",
      "expanding comments 0\n",
      "expanding comments 1\n",
      "expanding comments 2\n",
      "expanding comments 3\n",
      "expanding comments 4\n",
      "expanding comments 5\n",
      "expanding comments 6\n",
      "expanding comments 7\n",
      "expanding comments 8\n",
      "error https://www.facebook.com/ETtoday/posts/3437448309624642\n",
      "https://www.facebook.com/appledaily.tw/posts/10159431442147069\n",
      "expanding comments 0\n",
      "possilbe have 10.6 comments\n",
      "https://www.facebook.com/appledaily.tw/posts/10159029533387069\n",
      "expanding comments 0\n",
      "expanding comments 1\n",
      "possilbe have 58.6 comments\n",
      "https://www.facebook.com/news.ebc/posts/4274231742611884\n",
      "expanding comments 0\n",
      "expanding comments 1\n",
      "expanding comments 2\n",
      "expanding comments 3\n",
      "expanding comments 4\n",
      "expanding comments 5\n",
      "expanding comments 6\n",
      "expanding comments 7\n",
      "possilbe have 385.6 comments\n",
      "https://www.facebook.com/appledaily.tw/posts/10160133997867069\n",
      "expanding comments 0\n",
      "expanding comments 1\n",
      "expanding comments 2\n",
      "expanding comments 3\n",
      "expanding comments 4\n",
      "expanding comments 5\n",
      "expanding comments 6\n",
      "expanding comments 7\n",
      "expanding comments 8\n",
      "expanding comments 9\n",
      "expanding comments 10\n",
      "expanding comments 11\n",
      "possilbe have 561.2 comments\n",
      "https://www.facebook.com/appledaily.tw/posts/10159670713772069\n",
      "expanding comments 0\n",
      "possilbe have 46.8 comments\n",
      "https://www.facebook.com/ETtoday/posts/3577522222283916\n",
      "expanding comments 0\n",
      "expanding comments 1\n",
      "expanding comments 2\n",
      "expanding comments 3\n",
      "expanding comments 4\n",
      "possilbe have 214.2 comments\n",
      "https://www.facebook.com/ETtoday/posts/3542156359153836\n",
      "expanding comments 0\n",
      "expanding comments 1\n",
      "expanding comments 2\n",
      "expanding comments 3\n",
      "possilbe have 176.6 comments\n",
      "https://www.facebook.com/news.ebc/posts/5553901987978180\n",
      "expanding comments 0\n",
      "expanding comments 1\n",
      "expanding comments 2\n",
      "possilbe have 112.8 comments\n",
      "https://www.facebook.com/ETtoday/posts/3455359814500158\n",
      "expanding comments 0\n",
      "expanding comments 1\n",
      "expanding comments 2\n",
      "expanding comments 3\n",
      "expanding comments 4\n",
      "expanding comments 5\n",
      "expanding comments 6\n",
      "expanding comments 7\n",
      "expanding comments 8\n",
      "possilbe have 388.0 comments\n",
      "https://www.facebook.com/news.ebc/posts/4358407700860954\n",
      "expanding comments 0\n",
      "expanding comments 1\n",
      "expanding comments 2\n",
      "expanding comments 3\n",
      "expanding comments 4\n",
      "expanding comments 5\n",
      "expanding comments 6\n",
      "expanding comments 7\n",
      "possilbe have 365.4 comments\n"
     ]
    }
   ],
   "source": [
    "for link in postlink_ser.tolist():\n",
    "    splitted_link = link.split(\"/posts/\")\n",
    "    if splitted_link[0].split(\"/\")[-1]+\"-\"+splitted_link[-1] in os.listdir(\"./data/\"):\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            crawler.get_post_content(link)\n",
    "        except Exception as e:\n",
    "            print(f\"error {link}\")\n",
    "            print(e)\n",
    "            f = open(\"errlog.txt\", \"a\")\n",
    "            f.write(link+\"\\n\")\n",
    "            f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
